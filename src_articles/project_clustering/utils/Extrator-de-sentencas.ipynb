{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de extração de sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\15379\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\users\\15379\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: click in c:\\users\\15379\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\15379\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\15379\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\15379\\anaconda3\\lib\\site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\15379\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (0.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\15379\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\15379\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\15379\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.47.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\15379\\anaconda3\\lib\\site-packages (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\15379\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções e métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna a sentenca\n",
    "def get_Sentence(s):\n",
    "    resp = \"\"\n",
    "    for i in range(len(s)):\n",
    "        if(s[i] == '>'):\n",
    "            resp = s[i+1:]\n",
    "            break\n",
    "    resp = resp.strip()\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variáveis e constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diretorios dos datasets\n",
    "caminhos = ['cameras.xml','cells.xml','dvds.xml','laptops.xml','routers.xml']\n",
    "#Diretorios dos arquivos das sentencas\n",
    "save_sentencas = ['sentencas-camera.txt','sentencas-cells.txt','sentencas-dvds.txt','sentencas-laptops.txt','sentencas-routers.txt']\n",
    "#Diretorios dos arquivos de palavras-sentencas\n",
    "save_palavras = ['palavras-camera.txt','palavras-cells.txt','palavras-dvds.txt','palavras-laptops.txt','palavras-routers.txt']\n",
    "#Diretorios dos arquivos de frequencias de palavras\n",
    "save_frequencias = ['frequencias-camera.txt','frequencias-cells.txt','frequencias-dvds.txt','frequencias-laptops.txt','frequencias-routers.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leitura Dados: 100%|██████████| 1527/1527 [00:00<00:00, 10755.67it/s]\n",
      "Setencas: 100%|██████████| 246/246 [00:00<00:00, 124118.70it/s]\n",
      "Palavras e Setencas: 100%|██████████| 154/154 [00:00<00:00, 70354.30it/s]\n",
      "Frequencias: 100%|██████████| 154/154 [00:00<00:00, 10406.19it/s]\n",
      "Leitura Dados: 100%|██████████| 2395/2395 [00:00<00:00, 11300.12it/s]\n",
      "Setencas: 100%|██████████| 372/372 [00:00<00:00, 124048.42it/s]\n",
      "Palavras e Setencas: 100%|██████████| 237/237 [00:00<00:00, 120403.35it/s]\n",
      "Frequencias: 100%|██████████| 237/237 [00:00<00:00, 23689.86it/s]\n",
      "Leitura Dados: 100%|██████████| 978/978 [00:00<00:00, 9007.45it/s]\n",
      "Setencas: 100%|██████████| 159/159 [00:00<00:00, 107114.41it/s]\n",
      "Palavras e Setencas: 100%|██████████| 109/109 [00:00<00:00, 54824.22it/s]\n",
      "Frequencias: 100%|██████████| 109/109 [00:00<00:00, 54477.97it/s]\n",
      "Leitura Dados: 100%|██████████| 2488/2488 [00:00<00:00, 9431.82it/s]\n",
      "Setencas: 100%|██████████| 376/376 [00:00<00:00, 94366.82it/s]\n",
      "Palavras e Setencas: 100%|██████████| 259/259 [00:00<00:00, 115517.30it/s]\n",
      "Frequencias: 100%|██████████| 259/259 [00:00<00:00, 19917.58it/s]\n",
      "Leitura Dados: 100%|██████████| 1452/1452 [00:00<00:00, 10597.78it/s]\n",
      "Setencas: 100%|██████████| 237/237 [00:00<00:00, 118297.04it/s]\n",
      "Palavras e Setencas: 100%|██████████| 136/136 [00:00<00:00, 68029.26it/s]\n",
      "Frequencias: 100%|██████████| 136/136 [00:00<00:00, 27178.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#Para cada dominio\n",
    "for i in range(len(caminhos)):\n",
    "#for i in range(1):\n",
    "    sentencas = [] #Variavel que guarda as sentencas do documento\n",
    "    documentos = [] #Variavel que guarda os documentos do dominio\n",
    "    palavras = {} #Variavel que mapeia as sentencas para as palavras\n",
    "    sentenca = \"\"\n",
    "    palavra = \"\"\n",
    "    #Acessando os datasets\n",
    "    arq = open('../../datasets/' + caminhos[i],'r')\n",
    "    #Percorrendo as linhas do arquivo\n",
    "    for linha in tqdm(arq.readlines(), desc='Leitura Dados'):\n",
    "        if('<sentence' in linha):\n",
    "            s = linha.replace('</sentence>','')\n",
    "            s = s.split('idSentence=')[1]\n",
    "            s = get_Sentence(s) #Obtendo a sentenca\n",
    "            sentenca = s\n",
    "            sentencas.append(sentenca)\n",
    "        if('<opinion>' in linha):\n",
    "            p = linha.replace('/opinion','opinion')\n",
    "            p = p.replace('<opinion>','')\n",
    "            p = p.split('\"')\n",
    "            palavra = p[1].lower() #Obtendo o opinion (atributo)\n",
    "            tipo = p[-2]\n",
    "            if(tipo != 'anaphora' and (palavra not in set(stopwords.words('english')))):\n",
    "                if(palavra not in palavras.keys()):\n",
    "                    palavras[palavra] = set()\n",
    "                palavras[palavra].add(sentenca)\n",
    "        if('</review>' in linha):\n",
    "            sentenca = \"\"\n",
    "            palavra = \"\"\n",
    "            documentos.append(sentencas)\n",
    "            sentencas = []\n",
    "    arq.close()\n",
    "\n",
    "    #Salvando as sentencas num arquivo\n",
    "    arq = open('../../datasets_processed/sentencas/' + save_sentencas[i],'w')\n",
    "    for doc in tqdm(documentos, desc='Setencas'):\n",
    "        for sent in doc: \n",
    "            arq.write(sent + '\\n')\n",
    "        arq.write('\\n')\n",
    "    arq.close()\n",
    "\n",
    "    #Salvando as palavras com suas sentencas nos arquivos\n",
    "    arq = open('../../datasets_processed/' + save_palavras[i],'w')\n",
    "    for key in tqdm(sorted(palavras.keys()), desc='Palavras e Setencas'):\n",
    "        arq.write(key + ' ->> ') #Caracter separador de palavra e sentença\n",
    "        arq.write(str(palavras[key])+'\\n')\n",
    "    arq.close()\n",
    "\n",
    "    #Salvando as sentenças numa variavel\n",
    "    arq = open('../../datasets_processed/sentencas/' + save_sentencas[i],'r')\n",
    "    texto = arq.read().lower() #Obtem todos os documentos\n",
    "    arq.close()\n",
    "\n",
    "    #Salvando as frequencias nos arquivos\n",
    "    arq = open('../../datasets_processed/frequencias/' + save_frequencias[i],'w')\n",
    "    for p in tqdm(sorted(palavras.keys()), desc='Frequencias'):\n",
    "        frequencia = 0\n",
    "        '''if(' ' in p):\n",
    "            frequencia = texto.count(p)\n",
    "        else:\n",
    "            frequencia = TextBlob(texto).words.count(p)'''\n",
    "        frequencia = texto.count(p)\n",
    "        arq.write(p + ': ' + str(frequencia) + '\\n')\n",
    "    arq.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
